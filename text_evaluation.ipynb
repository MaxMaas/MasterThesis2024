{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIE 1 Text evaluation\n",
    "\n",
    "The code below can be used to calculate the BERTScore metrics. As input, the 10 generated solutions should be added to the cands variable, whereas the ground truth text should be added in the refs variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "import numpy as np\n",
    "\n",
    "# The candidate and reference sentences\n",
    "cands = [] # Add the cgenerated solution texts in this list\n",
    "refs = [] # Add the reference solution text in this list\n",
    "\n",
    "\n",
    "# Calculate the BERTScore for each candidate with the reference sentence\n",
    "P_scores = []\n",
    "R_scores = []\n",
    "F1_scores = []\n",
    "for cand in cands:\n",
    "    P, R, F1 = score([cand], [refs], lang='en', verbose=True)\n",
    "    P_scores.append(P)\n",
    "    R_scores.append(R)\n",
    "    F1_scores.append(F1)\n",
    "\n",
    "# Print the performance metrics\n",
    "for i, (P, R, F1) in enumerate(zip(P_scores, R_scores, F1_scores)):\n",
    "    print(f'Candidate {i+1}:\\nPrecision: {P}\\nRecall: {R}\\nF1 Score: {F1}\\n')\n",
    "\n",
    "print(f'avg precision: {np.mean(P_scores)} \\n avg recall: {np.mean(R_scores)} \\n avg F1: {np.mean(F1_scores)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
